{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87d1d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 116, 116, 112, 115, 37, 51, 65, 47, 47, 101, 120, 97, 109, 112, 108, 101, 46, 99, 111, 109, 47, 112, 97, 116, 104, 37, 51, 70, 112, 97, 114, 97, 109, 49, 37, 51, 68, 118, 97, 108, 117, 101, 49, 37, 50, 54, 112, 97, 114, 97, 109, 50, 37, 51, 68, 118, 97, 108, 117, 101, 50]\n",
      "num_train:  67139\n",
      "num_cv:  14386\n",
      "num_test:  14386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\AppData\\Local\\Temp\\ipykernel_22868\\2993620996.py:41: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  urldata = pd.read_csv('C:\\\\Users\\\\karan\\\\Downloads\\\\urldata.csv', encoding = \"ISO-8859-1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 119, 119, 119, 46, 115, 101, 114, 118, 101, 114, 98, 114, 115, 112, 46, 99, 111, 109, 46, 98, 114, 47, 115, 101, 110, 100, 101, 114, 47, 114, 101, 109, 46, 112, 104, 112], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 119, 119, 119, 46, 112, 97, 117, 108, 103, 114, 97, 104, 97, 109, 46, 99, 111, 109, 47, 119, 101, 98, 50, 48, 46, 104, 116, 109, 108], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 109, 111, 122, 46, 111, 114, 103, 47, 97, 110, 110, 46, 104, 116, 109, 108], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 116, 111, 111, 108, 115, 46, 105, 101, 116, 102, 46, 111, 114, 103, 47, 104, 116, 109, 108, 47, 114, 102, 99, 50, 57, 52, 49], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99, 104, 97, 116, 51, 97, 46, 108, 105, 118, 101, 99, 104, 97, 116, 105, 110, 99, 46, 99, 111, 109, 47, 108, 105, 99, 101, 110, 99, 101, 47, 49, 54, 55, 52, 53, 50, 49, 47, 111, 112, 101, 110, 95, 99, 104, 97, 116, 46, 99, 103, 105, 37, 51, 70, 108, 97, 110, 103, 37, 51, 68, 101, 110, 37, 50, 54, 103, 114, 111, 117, 112, 115, 37, 51, 68, 48]]\n",
      "Shape of X_train:  (67139, 200)\n",
      "Shape of y_train:  (67139, 1)\n",
      "Epoch 1/10\n",
      "2099/2099 [==============================] - 49s 23ms/step - loss: 0.3880 - val_loss: 0.2916\n",
      "Epoch 2/10\n",
      "2099/2099 [==============================] - 46s 22ms/step - loss: 0.2779 - val_loss: 0.2667\n",
      "Epoch 3/10\n",
      "2099/2099 [==============================] - 42s 20ms/step - loss: 0.2514 - val_loss: 0.2615\n",
      "Epoch 4/10\n",
      "2099/2099 [==============================] - 42s 20ms/step - loss: 0.2406 - val_loss: 0.2579\n",
      "Epoch 5/10\n",
      "2099/2099 [==============================] - 44s 21ms/step - loss: 0.2308 - val_loss: 0.2812\n",
      "Epoch 6/10\n",
      "2099/2099 [==============================] - 47s 22ms/step - loss: 0.2264 - val_loss: 0.2560\n",
      "Epoch 7/10\n",
      "2099/2099 [==============================] - 47s 22ms/step - loss: 0.2239 - val_loss: 0.2610\n",
      "Epoch 8/10\n",
      "2099/2099 [==============================] - 47s 22ms/step - loss: 0.2211 - val_loss: 0.2590\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Embedding, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Previous pre-processing unit \n",
    "def convert_url(raw_url):\n",
    "    # Tokenize the URL\n",
    "    tokenized_url = urllib.parse.quote(raw_url)\n",
    "\n",
    "    # Standardize the URL\n",
    "    standardized_url = urllib.parse.urlsplit(tokenized_url).geturl()\n",
    "\n",
    "    # Truncate or pad the URL\n",
    "    max_url_length = 200  # Maximum length of the padded URL\n",
    "\n",
    "    if len(standardized_url) > max_url_length:\n",
    "        # Truncate the URL if it is longer than the maximum length\n",
    "        truncated_url = standardized_url[-max_url_length:]\n",
    "        padded_url = [ord(char) for char in truncated_url]\n",
    "    else:\n",
    "        # Pad the URL with zeros if it is shorter than the maximum length\n",
    "        padded_url = [0] * (max_url_length - len(standardized_url)) + [ord(char) for char in standardized_url]\n",
    "    \n",
    "    return padded_url\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "raw_url = \"https://example.com/path?param1=value1&param2=value2\"\n",
    "padded_url = convert_url(raw_url)\n",
    "print(padded_url)\n",
    "\n",
    "# Reading the training data\n",
    "urldata = pd.read_csv('C:\\\\Users\\\\karan\\\\Downloads\\\\urldata.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# taking only required columns into a new Dataframe\n",
    "urldata = urldata.filter(['domain','label'],axis=1)\n",
    "\n",
    "# Shuffle input randomly and then take ratio 0.7, 0.15, 0.15\n",
    "data = urldata.sample(frac = 1, random_state = 9)\n",
    "\n",
    "# Split the data into training, cross-validation and test datasets\n",
    "train_ratio = 0.7\n",
    "cv_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Compute number of each type of data \n",
    "total = len(data)\n",
    "num_train = int(total * train_ratio)\n",
    "num_cv = int(total * cv_ratio)\n",
    "num_test = int(total * test_ratio)\n",
    "\n",
    "# Split data into three\n",
    "train_data = data[:num_train]\n",
    "val_data = data[num_train:num_train + num_cv]\n",
    "test_data = data[num_train + num_cv:]\n",
    "\n",
    "print(\"num_train: \", num_train)\n",
    "print(\"num_cv: \", num_cv)\n",
    "print(\"num_test: \", num_test)\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "# Get the X_train, y_train, X_cv, y_cv, X_test, y_test\n",
    "X_train = train_data['domain'].values\n",
    "y_train = train_data['label'].values\n",
    "X_train = list(X_train)\n",
    "y_train = list(y_train)\n",
    "X_cv = val_data['domain'].values\n",
    "y_cv = val_data['label'].values\n",
    "X_cv = list(X_cv)\n",
    "y_cv = list(y_cv)\n",
    "X_test = test_data['domain'].values\n",
    "y_test = test_data['label'].values\n",
    "X_test = list(X_test)\n",
    "y_test = list(y_test)\n",
    "#print(X_train[:5])\n",
    "#print(y_train[:5])\n",
    "\n",
    "# Convert the URLs into padded URLs using a function(to be vectorized)\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = convert_url(X_train[i])\n",
    "for i in range(len(X_cv)):\n",
    "    X_cv[i] = convert_url(X_cv[i])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = convert_url(X_test[i])\n",
    "print(X_train[:5])\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i] = [y_train[i]]\n",
    "for i in range(len(y_cv)):\n",
    "    y_cv[i] = [y_cv[i]]\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = [y_test[i]]\n",
    "\n",
    "# Convert the inputs to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_cv = np.array(X_cv)\n",
    "y_cv = np.array(y_cv)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "\n",
    "# Model making\n",
    "inp_dim = 256\n",
    "emb_dim = 32\n",
    "seq_len = 200\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = inp_dim, output_dim = emb_dim, input_length = seq_len),\n",
    "    Flatten(),\n",
    "    Dense(units = 512, activation = 'relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(units = 256, activation = 'relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(units = 1, activation = 'linear', kernel_regularizer=regularizers.l2(0.001))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    ")\n",
    "\n",
    "# Define early stopping callback\n",
    "# patience - number of epochs to wait for a significant improvement in the loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_cv, y_cv), epochs = 10, callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427e4cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099/2099 [==============================] - 6s 3ms/step\n",
      "Accuracy on training set:  [0.94760125]\n",
      "14386\n",
      "450/450 [==============================] - 2s 4ms/step\n",
      "Accuracy on cross-validation set:  [0.92026971]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy obtained on training and cross-validation set\n",
    "output = model.predict(X_train)\n",
    "for i in range(len(output)):\n",
    "    if output[i] >= 0.5:\n",
    "        output[i] = 1\n",
    "    else:\n",
    "        output[i] = 0\n",
    "output = np.array(output)\n",
    "res = sum(y_train == output)/len(y_train)\n",
    "print(\"Accuracy on training set: \", res)\n",
    "print(len(y_cv))\n",
    "\n",
    "output = model.predict(X_cv)\n",
    "for i in range(len(output)):\n",
    "    if output[i] >= 0.5:\n",
    "        output[i] = 1\n",
    "    else:\n",
    "        output[i] = 0\n",
    "output = np.array(output)\n",
    "res = sum(y_cv == output)/len(y_cv)\n",
    "print(\"Accuracy on cross-validation set: \", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a212ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
